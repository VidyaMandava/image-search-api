# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15nKeIJTVubr65xCEIVUF4V0z1JoeTHvG
"""

# Lightweight FastAPI - Memory Optimized
import subprocess
import sys
import os
import signal
import time
import socket
import gc
import psutil

print("🧹 Initial cleanup and memory check...")

# Check available memory
memory = psutil.virtual_memory()
print(f"💾 Available memory: {memory.available / (1024**3):.1f} GB")
print(f"💾 Total memory: {memory.total / (1024**3):.1f} GB")

if memory.available < 2 * (1024**3):  # Less than 2GB
    print("⚠️  Low memory detected - using minimal model setup")
    USE_MINIMAL_MODELS = True
else:
    USE_MINIMAL_MODELS = False

def cleanup_processes():
    """Kill existing servers"""
    try:
        for port in range(8000, 8010):
            try:
                result = subprocess.run(['lsof', '-ti', f':{port}'], capture_output=True, text=True)
                if result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        if pid:
                            try:
                                os.kill(int(pid), signal.SIGKILL)
                            except:
                                pass
            except:
                pass
        time.sleep(1)
        print("✅ Cleanup complete")
    except Exception as e:
        print(f"Cleanup warning: {e}")

cleanup_processes()

# Force garbage collection
gc.collect()

# Install minimal packages first
print("📦 Installing minimal packages...")
minimal_packages = ['fastapi', 'uvicorn', 'pyngrok', 'requests', 'pillow']

for package in minimal_packages:
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except:
        print(f"Failed to install {package}")

print("✅ Minimal packages installed")

# Create a very lightweight FastAPI server first
lightweight_server_code = '''
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import time
import logging
import uvicorn
import socket
import os
import json
import gc
import psutil

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_free_port(start_port=8000):
    """Find a free port"""
    for port in range(start_port, start_port + 10):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            sock.bind(('', port))
            sock.close()
            return port
        except OSError:
            continue
    return start_port

SERVER_PORT = find_free_port()
print(f"🚀 Using port: {SERVER_PORT}")

# Memory check
memory = psutil.virtual_memory()
print(f"💾 Memory available: {memory.available / (1024**3):.1f} GB")

# Get Colab secrets safely
def get_colab_secret(key, default=""):
    try:
        from google.colab import userdata
        return userdata.get(key)
    except Exception as e:
        print(f"Could not get secret {key}: {e}")
        return default

# Configuration
class Config:
    def __init__(self):
        self.AZURE_STORAGE_CONNECTION_STRING = get_colab_secret('AZURE_STORAGE_CONNECTION_STRING')
        self.REDIS_PASSWORD = get_colab_secret('Azure_Redis_Cache')
        self.CONTAINER_NAME = "images"
        self.REDIS_HOST = "redis-11468.c282.east-us-mz.azure.redns.redis-cloud.com"
        self.REDIS_PORT = 11468

config = Config()

# Initialize FastAPI app
app = FastAPI(
    title="Image Search API - Lightweight",
    description="Memory-optimized image search system",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables for lazy loading
clients = {}
models = {}
services_initialized = False

# Pydantic models
class SearchRequest(BaseModel):
    query: str = Field(..., description="Search query for images")
    max_results: int = Field(default=5, ge=1, le=10, description="Maximum number of results")
    include_explanations: bool = Field(default=False, description="Include AI explanations")
    use_cache: bool = Field(default=True, description="Use cached results if available")

class SearchResult(BaseModel):
    rank: int
    image_name: str
    explanation: Optional[str] = None
    similarity_score: Optional[float] = None
    image_url: Optional[str] = None

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]
    total_results: int
    processing_time: float

class HealthResponse(BaseModel):
    status: str
    services: Dict[str, str]
    memory_usage: Dict[str, float]
    port: int

def init_redis():
    """Initialize Redis connection"""
    if clients.get('redis'):
        return clients['redis']

    try:
        import redis
        if not config.REDIS_PASSWORD:
            logger.warning("No Redis password provided")
            return None

        r = redis.StrictRedis(
            host=config.REDIS_HOST,
            port=config.REDIS_PORT,
            password=config.REDIS_PASSWORD,
            decode_responses=True,
            ssl=False,
            socket_connect_timeout=3,
            socket_timeout=3
        )
        r.ping()
        clients['redis'] = r
        logger.info("✅ Redis connected")
        return r
    except Exception as e:
        logger.error(f"Redis connection failed: {e}")
        return None

def init_azure():
    """Initialize Azure Blob Storage"""
    if clients.get('blob_service'):
        return clients['blob_service']

    try:
        if not config.AZURE_STORAGE_CONNECTION_STRING:
            logger.warning("No Azure connection string provided")
            return None

        from azure.storage.blob import BlobServiceClient
        blob_service = BlobServiceClient.from_connection_string(config.AZURE_STORAGE_CONNECTION_STRING)
        container = blob_service.get_container_client(config.CONTAINER_NAME)

        # Test connection
        container.get_account_information()

        clients['blob_service'] = blob_service
        clients['container'] = container
        logger.info("✅ Azure Blob Storage connected")
        return blob_service
    except Exception as e:
        logger.error(f"Azure connection failed: {e}")
        return None

def init_chromadb():
    """Initialize ChromaDB"""
    if clients.get('collection'):
        return clients['collection']

    try:
        import chromadb
        client = chromadb.PersistentClient(path="chromadb_persist")
        collection = client.get_or_create_collection("image_embeddings")
        clients['chroma'] = client
        clients['collection'] = collection
        logger.info("✅ ChromaDB connected")
        return collection
    except Exception as e:
        logger.error(f"ChromaDB connection failed: {e}")
        return None

def load_minimal_models():
    """Load only essential models"""
    if models.get('clip_loaded'):
        return True

    try:
        # Check memory before loading
        memory = psutil.virtual_memory()
        if memory.available < 1.5 * (1024**3):  # Less than 1.5GB
            logger.warning("Insufficient memory for models - using demo mode")
            return False

        logger.info("🤖 Loading CLIP model...")
        from transformers import CLIPProcessor, CLIPModel
        import torch

        models['clip_model'] = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        models['clip_processor'] = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        models['clip_loaded'] = True

        # Force garbage collection after loading
        gc.collect()

        logger.info("✅ CLIP model loaded")
        return True
    except Exception as e:
        logger.error(f"Failed to load models: {e}")
        return False

def search_demo_images(query: str, max_results: int = 5):
    """Demo search function when services aren't available"""
    demo_images = [
        "sunset_mountain_1.jpg",
        "ocean_waves_2.jpg",
        "forest_landscape_3.jpg",
        "city_skyline_4.jpg",
        "beach_sunset_5.jpg"
    ]

    return demo_images[:max_results]

def search_with_chromadb(query: str, max_results: int = 5):
    """Search using ChromaDB if available"""
    try:
        collection = init_chromadb()
        if not collection:
            return search_demo_images(query, max_results)

        # Check if we have models loaded
        if not models.get('clip_loaded'):
            if not load_minimal_models():
                return search_demo_images(query, max_results)

        # Generate query embedding
        import torch
        import numpy as np

        inputs = models['clip_processor'](text=[query], return_tensors="pt", padding=True)
        with torch.no_grad():
            query_embedding = models['clip_model'].get_text_features(**inputs)

        # Search in ChromaDB
        results = collection.query(
            query_embeddings=query_embedding.cpu().numpy().tolist(),
            n_results=max_results
        )

        if results['documents'] and len(results['documents']) > 0:
            return results['documents'][0]

        return search_demo_images(query, max_results)

    except Exception as e:
        logger.error(f"Search failed: {e}")
        return search_demo_images(query, max_results)

# API Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check with memory monitoring"""
    memory = psutil.virtual_memory()

    services = {}

    # Check Redis
    redis_client = init_redis()
    services['redis'] = "healthy" if redis_client else "unavailable"

    # Check Azure
    azure_client = init_azure()
    services['azure_blob'] = "healthy" if azure_client else "unavailable"

    # Check ChromaDB
    chroma_collection = init_chromadb()
    services['chromadb'] = "healthy" if chroma_collection else "unavailable"

    # Check models
    services['models'] = "loaded" if models.get('clip_loaded') else "not_loaded"

    return HealthResponse(
        status="healthy",
        services=services,
        memory_usage={
            "available_gb": round(memory.available / (1024**3), 2),
            "used_percent": memory.percent
        },
        port=SERVER_PORT
    )

@app.post("/search", response_model=SearchResponse)
async def search_images(request: SearchRequest):
    """Search for images"""
    start_time = time.time()

    try:
        logger.info(f"🔍 Searching for: {request.query}")

        # Perform search
        image_names = search_with_chromadb(request.query, request.max_results)

        # Build results
        results = []
        for i, image_name in enumerate(image_names):
            result = SearchResult(
                rank=i + 1,
                image_name=image_name,
                similarity_score=max(0.5, 1.0 - (i * 0.1)),
                image_url=f"/images/{image_name}"
            )

            if request.include_explanations:
                result.explanation = f"This image appears relevant to '{request.query}' based on visual similarity."

            results.append(result)

        processing_time = time.time() - start_time
        logger.info(f"✅ Search completed in {processing_time:.2f}s")

        return SearchResponse(
            query=request.query,
            results=results,
            total_results=len(results),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f"❌ Search error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/init-models")
async def initialize_models():
    """Endpoint to load models on demand"""
    try:
        success = load_minimal_models()
        if success:
            return {"status": "success", "message": "Models loaded successfully"}
        else:
            return {"status": "failed", "message": "Insufficient memory for models"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/memory")
async def memory_status():
    """Check memory usage"""
    memory = psutil.virtual_memory()
    return {
        "available_gb": round(memory.available / (1024**3), 2),
        "total_gb": round(memory.total / (1024**3), 2),
        "used_percent": memory.percent,
        "models_loaded": bool(models.get('clip_loaded'))
    }

@app.get("/")
async def root():
    """Root endpoint"""
    memory = psutil.virtual_memory()
    return {
        "message": "Image Search API - Lightweight Version",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "init_models": "/init-models",
        "memory": "/memory",
        "port": SERVER_PORT,
        "memory_available_gb": round(memory.available / (1024**3), 2)
    }

if __name__ == "__main__":
    logger.info(f"🚀 Starting lightweight server on port {SERVER_PORT}")
    uvicorn.run(app, host="0.0.0.0", port=SERVER_PORT, log_level="info")
'''

print("📝 Creating lightweight server...")
with open("/tmp/lightweight_fastapi.py", "w") as f:
    f.write(lightweight_server_code)

print("✅ Lightweight server file created")

# Start the lightweight server
def start_lightweight_server():
    print("🚀 Starting lightweight FastAPI server...")

    try:
        process = subprocess.Popen([
            sys.executable, "/tmp/lightweight_fastapi.py"
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        return process
    except Exception as e:
        print(f"Failed to start process: {e}")
        return None

# Main execution
print("\n" + "=" * 60)
print("🚀 STARTING LIGHTWEIGHT FASTAPI SERVER")
print("=" * 60)

server_process = start_lightweight_server()

if server_process:
    print("⏳ Waiting for server to start...")
    time.sleep(8)

    # Check if server started successfully
    import requests
    server_port = None

    for port in range(8000, 8010):
        try:
            response = requests.get(f"http://localhost:{port}/", timeout=5)
            if response.status_code == 200:
                server_port = port
                print(f"✅ Server running on port {port}")
                print(f"Response: {response.json()}")
                break
        except Exception as e:
            continue

    if server_port:
        # Check memory status
        try:
            memory_response = requests.get(f"http://localhost:{server_port}/memory", timeout=5)
            print(f"💾 Memory status: {memory_response.json()}")
        except:
            pass

        # Setup ngrok
        try:
            from pyngrok import ngrok
            ngrok.kill()
            public_url = ngrok.connect(server_port)

            print(f"\n🎉 SUCCESS! Lightweight API is running!")
            print(f"🌐 Public URL: {public_url}")
            print(f"📚 Documentation: {public_url}/docs")
            print(f"🔍 Health Check: {public_url}/health")
            print(f"💾 Memory Status: {public_url}/memory")
            print(f"🤖 Load Models: {public_url}/init-models")

            # Test basic functionality
            try:
                test_response = requests.post(f"{public_url}/search",
                    json={"query": "sunset", "max_results": 3, "include_explanations": False},
                    timeout=15)
                print(f"✅ Basic search test: {test_response.status_code}")
            except Exception as e:
                print(f"⚠️  Search test: {e}")

        except Exception as e:
            print(f"⚠️  Ngrok setup failed: {e}")
            print(f"Server running locally at: http://localhost:{server_port}")
    else:
        print("❌ Server failed to start properly")

        # Check process output
        if server_process:
            try:
                stdout, stderr = server_process.communicate(timeout=5)
                if stdout:
                    print("Server output:", stdout)
                if stderr:
                    print("Server errors:", stderr)
            except:
                print("Could not get server output")
else:
    print("❌ Failed to start server process")

print("\n" + "=" * 60)
print("NEXT STEPS:")
print("1. Visit /docs for API documentation")
print("2. Use /health to check service status")
print("3. Use /init-models to load ML models when needed")
print("4. Use /search to search for images")
print("=" * 60)